{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric, ClassLabel\n",
    "import random\n",
    "import pandas as pd\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "\n",
    "\n",
    "train_set = \"train.csv\"\n",
    "dev_set = \"dev.csv\"\n",
    "info_file = \"info.json\"\n",
    "# dir_path = os.path.dirname(os.path.abspath(__file__))\n",
    "dir_path = 'D:/MRC/MRC_CSK_pretraining/dataset'\n",
    "cls_task_token = \"madeupword0002\"\n",
    "mc_token=\"madeupword0001\"\n",
    "span_token=\"madeupword0000\"\n",
    "task_types = [\"span\", \"mc\", \"cls\"]\n",
    "\n",
    "train_key = \"train\"\n",
    "eval_key = \"eval\"\n",
    "\n",
    "class Dataset :\n",
    "    def __init__(self, task, dataroot, split, task_type, task_category, task_choices, max_seq_length, tokenizer):\n",
    "        path = dir_path + \"/\" + dataroot + '/'\n",
    "        self.datasets = load_dataset('csv', data_files={train_key: path + train_set, eval_key : path + dev_set})\n",
    "        self.configs = None\n",
    "        with open(path + info_file) as json_file:\n",
    "            self.configs = json.load(json_file)\n",
    "        self.task = task\n",
    "        self.task_type = task_type\n",
    "        self.task_category = task_category\n",
    "        self.split = split\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.task_choices = task_choices\n",
    "\n",
    "    def __call__(self):\n",
    "        self.datasets = self.datasets.map(self.preprocess_function, batched=True)\n",
    "        return self.datasets\n",
    "\n",
    "class CoLADataset(Dataset) :\n",
    "    def __init__(self, task, dataroot, split, task_type, task_category, task_choices, max_seq_length, tokenizer):\n",
    "        super().__init__(task, dataroot, split, task_type, task_category, task_choices, max_seq_length, tokenizer)\n",
    "\n",
    "    def preprocess_function(self, examples):\n",
    "        sentences = [cls_task_token + sentence for sentence in examples[\"sentence\"]]\n",
    "        examples['class'] = [self.task] * len(examples[\"sentence\"])\n",
    "        tokenized_examples = self.tokenizer(sentences, truncation=True)\n",
    "\n",
    "        return {k : v for k, v in tokenized_examples.items()}\n",
    "\n",
    "class CommonsenseQADataset(Dataset) :\n",
    "    def __init__(self, task, dataroot, split, task_type, task_category, task_choices, max_seq_length, tokenizer):\n",
    "        super().__init__(task, dataroot, split, task_type, task_category, task_choices, max_seq_length, tokenizer)\n",
    "        self.choice_names = [\"answerA\", \"answerB\", \"answerC\", \"answerD\", \"answerE\"]\n",
    "        \n",
    "    def preprocess_function(self, examples):\n",
    "        first_sentences = [[mc_token + question] * 5 for i, question in enumerate(examples[\"question\"])]\n",
    "        question_headers = examples[\"question\"]\n",
    "        second_sentences = [[f\"{examples[choice][i]}\" for choice in self.choice_names] for i, header in enumerate(question_headers)]\n",
    "        examples['class'] = [self.task] * len(examples[\"question\"])\n",
    "\n",
    "        # Flatten\n",
    "        first_sentences = sum(first_sentences, [])\n",
    "        second_sentences = sum(second_sentences, [])\n",
    "        \n",
    "        tokenized_examples = self.tokenizer(first_sentences, second_sentences, truncation=True)\n",
    "\n",
    "        # Un-flatten\n",
    "        mapped_result = {k: [v[i:i+5] for i in range(0, len(v), 5)] for k, v in tokenized_examples.items()}\n",
    "        return mapped_result\n",
    "\n",
    "class SocialIQADataset(Dataset) :\n",
    "    def __init__(self, task, dataroot, split, task_type, task_category, task_choices, max_seq_length, tokenizer):\n",
    "        super().__init__(task, dataroot, split, task_type, task_category, task_choices, max_seq_length, tokenizer)\n",
    "        self.choice_names = [\"answerA\", \"answerB\", \"answerC\"]\n",
    "        \n",
    "    def preprocess_function(self, examples):\n",
    "        first_sentences = [[mc_token + question] * 3 for i, question in enumerate(examples[\"Context\"])]\n",
    "        question_headers = examples[\"Context\"]\n",
    "        second_sentences = [[f\"{examples[choice][i]}\" for choice in self.choice_names] for i, header in enumerate(question_headers)]\n",
    "        examples['class'] = [self.task] * len(examples[\"Context\"])\n",
    "\n",
    "        # Flatten\n",
    "        first_sentences = sum(first_sentences, [])\n",
    "        second_sentences = sum(second_sentences, [])\n",
    "        \n",
    "        tokenized_examples = self.tokenizer(first_sentences, second_sentences, truncation=True)\n",
    "\n",
    "        # Un-flatten\n",
    "        mapped_result = {k: [v[i:i+3] for i in range(0, len(v), 3)] for k, v in tokenized_examples.items()}\n",
    "        return mapped_result\n",
    "\n",
    "class MultiRCDataset(Dataset) :\n",
    "    #passage,question,answer,label\n",
    "    def __init__(self, task, dataroot, split, task_type, task_category, task_choices, max_seq_length, tokenizer):\n",
    "        super().__init__(task, dataroot, split, task_type, task_category, task_choices, max_seq_length, tokenizer)\n",
    "        \n",
    "    def preprocess_function(self, examples):\n",
    "        passage_question = [cls_task_token + examples[\"question\"][i] + \"\\n\" + passage  for i, passage in enumerate(examples[\"passage\"])]\n",
    "        second_answer = [str(answer) for answer in examples[\"answer\"]]\n",
    "        examples['class'] = [self.task] * len(examples[\"question\"])\n",
    "\n",
    "        tokenized_examples = self.tokenizer(passage_question, second_answer, truncation=True)\n",
    "        return {k : v for k, v in tokenized_examples.items()}\n",
    "\n",
    "class SQuadDataset(Dataset) :\n",
    "    def __init__(self, task, dataroot, split, task_type, task_category, task_choices, max_seq_length, tokenizer) :\n",
    "        super().__init__(task, dataroot, split, task_type, task_category, task_choices, max_seq_length, tokenizer) \n",
    "    \n",
    "    def preprocess_function(self, examples) :\n",
    "        examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "        context_question = [span_token + examples[\"question\"][i] + \"\\n\" + context for i, context in enumerate(examples[\"context\"])]\n",
    "\n",
    "        tokenized_examples = tokenizer(\n",
    "            context_question,\n",
    "            truncation=True,\n",
    "            max_length=max_seq_length,\n",
    "            return_overflowing_tokens=True, # tokens overlapeed with doc stride\n",
    "            return_offsets_mapping=True, # \n",
    "        )\n",
    "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\") \n",
    "        offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "        # Let's label those examples!\n",
    "        tokenized_examples[\"start_positions\"] = []\n",
    "        tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "        for i, offsets in enumerate(offset_mapping):\n",
    "            input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "            cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "            sample_index = sample_mapping[i]\n",
    "            answers = eval(examples[\"answers\"][sample_index])\n",
    "            \n",
    "            # If no answers are given, set the cls_index as answer.\n",
    "            if len(answers[\"answer_start\"]) == 0:\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Start/end character index of the answer in the text.\n",
    "                start_char = answers[\"answer_start\"][0]\n",
    "                end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "                # Start token index of the current span in the text.\n",
    "                token_start_index = 0\n",
    "                while sequence_ids[token_start_index] != 0 :\n",
    "                    token_start_index += 1\n",
    "\n",
    "                # End token index of the current span in the text.\n",
    "                token_end_index = len(input_ids) - 1\n",
    "                while sequence_ids[token_end_index] != 0 :\n",
    "                    token_end_index -= 1\n",
    "\n",
    "                # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                    tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                    tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "                else:\n",
    "                    # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                    # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                        token_start_index += 1\n",
    "                    tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                    while offsets[token_end_index][1] >= end_char:\n",
    "                        token_end_index -= 1\n",
    "                    tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "        return tokenized_examples\n",
    "\n",
    "        \n",
    "\n",
    "DatasetFactory = {\n",
    "    \"CoLA\": CoLADataset,\n",
    "    \"MultiRC\": MultiRCDataset,\n",
    "    \"SocialIQA\": SocialIQADataset,\n",
    "    \"CommonsenseQA\": CommonsenseQADataset,\n",
    "    \"Squad1.1\": SQuadDataset\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-671bad6838d17224\n",
      "Reusing dataset csv (C:\\Users\\user\\.cache\\huggingface\\datasets\\csv\\default-671bad6838d17224\\0.0.0\\9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('csv', data_files='./mrc/squad1.1/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(self, examples) :\n",
    "        examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "        context_question = [span_token + examples[\"question\"][i] + \"\\n\" + context for i, context in enumerate(examples[\"context\"])]\n",
    "\n",
    "        tokenized_examples = tokenizer(\n",
    "            context_question,\n",
    "            truncation=True,\n",
    "            max_length=max_seq_length,\n",
    "            return_overflowing_tokens=True, # tokens overlapeed with doc stride\n",
    "            return_offsets_mapping=True, # \n",
    "        )\n",
    "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\") \n",
    "        offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "        # Let's label those examples!\n",
    "        tokenized_examples[\"start_positions\"] = []\n",
    "        tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "        for i, offsets in enumerate(offset_mapping):\n",
    "            input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "            cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "            sample_index = sample_mapping[i]\n",
    "            answers = eval(examples[\"answers\"][sample_index])\n",
    "            \n",
    "            # If no answers are given, set the cls_index as answer.\n",
    "            if len(answers[\"answer_start\"]) == 0:\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Start/end character index of the answer in the text.\n",
    "                start_char = answers[\"answer_start\"][0]\n",
    "                end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "                # Start token index of the current span in the text.\n",
    "                token_start_index = 0\n",
    "                while sequence_ids[token_start_index] != 0 :\n",
    "                    token_start_index += 1\n",
    "\n",
    "                # End token index of the current span in the text.\n",
    "                token_end_index = len(input_ids) - 1\n",
    "                while sequence_ids[token_end_index] != 0 :\n",
    "                    token_end_index -= 1\n",
    "\n",
    "                # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                    tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                    tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "                else:\n",
    "                    # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                    # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                        token_start_index += 1\n",
    "                    tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                    while offsets[token_end_index][1] >= end_char:\n",
    "                        token_end_index -= 1\n",
    "                    tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "        return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from transformers import RobertaTokenizerFast, RobertaTokenizer, AddedToken\n",
    "\n",
    "class RobertaMuppetTokenizerFast(RobertaTokenizerFast):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_file,\n",
    "        merges_file,\n",
    "        errors=\"replace\",\n",
    "        bos_token=\"<s>\",\n",
    "        eos_token=\"</s>\",\n",
    "        sep_token=\"</s>\",\n",
    "        cls_token=\"<s>\",\n",
    "        unk_token=\"<unk>\",\n",
    "        pad_token=\"<pad>\",\n",
    "        mask_token=\"<mask>\",\n",
    "        span_token=\"madeupword0000\",\n",
    "        mc_token=\"madeupword0001\",\n",
    "        cls_task_token = \"madeupword0002\",\n",
    "        add_prefix_space=False,\n",
    "        **kwargs):\n",
    "        mrc_token = AddedToken(span_token, lstrip=False, rstrip=False) if isinstance(span_token, str) else span_token\n",
    "        com_token = AddedToken(mc_token, lstrip=False, rstrip=False) if isinstance(mc_token, str) else mc_token\n",
    "        cls_task_token = AddedToken(cls_task_token, lstrip=False, rstrip=False) if isinstance(cls_task_token, str) else cls_task_token\n",
    "        special_tokens_dict  = { \"additional_special_tokens\" : [mrc_token, com_token, cls_task_token]}\n",
    "        \n",
    "        super().__init__(\n",
    "            vocab_file=vocab_file,\n",
    "            merges_file=merges_file,\n",
    "            errors=errors,\n",
    "            bos_token=bos_token,\n",
    "            eos_token=eos_token,\n",
    "            unk_token=unk_token,\n",
    "            sep_token=sep_token,\n",
    "            cls_token=cls_token,\n",
    "            pad_token=pad_token,\n",
    "            mask_token=mask_token,\n",
    "            cls_task_token=cls_task_token,\n",
    "            add_prefix_space=add_prefix_space,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.add_special_tokens(special_tokens_dict)\n",
    "        #model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    def build_input_with_special_tokens(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        print(\"is silhang?\")\n",
    "        cls = [self.cls_token_id]\n",
    "        sep = [self.sep_token_id]\n",
    "        if len(token_ids_0) > 0 and token_ids_0[0] in self.additional_special_tokens_ids:\n",
    "            if token_ids_1 is None:\n",
    "                return token_ids_0 + [self.sep_token_id]\n",
    "            return token_ids_0 + sep + sep + token_ids_1 + sep\n",
    "            \n",
    "        if token_ids_1 is None:\n",
    "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
    "        \n",
    "        return cls + token_ids_0 + sep + sep + token_ids_1 + sep\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'RobertaMuppetTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaMuppetTokenizerFast.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'RobertaMuppetTokenizer'.\n",
      "Using custom data configuration default-9e7326804edaf423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to C:\\Users\\user\\.cache\\huggingface\\datasets\\csv\\default-9e7326804edaf423\\0.0.0\\9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:\\Users\\user\\.cache\\huggingface\\datasets\\csv\\default-9e7326804edaf423\\0.0.0\\9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79e5a27ef9074bbe99be2a93865ade50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=88.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02edbf4fba1b49afa08a595c0380aca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0, 50261, 3972, 2661, 222, 5, 9880, 2708, 2346, 2082, 11, 504, 4432, 11, 226, 2126, 10067, 1470, 116, 2, 2, 37848, 37471, 28108, 6, 5, 334, 34, 10, 4019, 2048, 4, 497, 1517, 5, 4326, 6919, 18, 1637, 31346, 16, 10, 9030, 9577, 9, 5, 9880, 2708, 4, 29261, 11, 760, 9, 5, 4326, 6919, 8, 2114, 24, 6, 16, 10, 7621, 9577, 9, 4845, 19, 3701, 62, 33161, 19, 5, 7875, 22, 39043, 1459, 1614, 1464, 13292, 4977, 845, 4130, 7, 5, 4326, 6919, 16, 5, 26429, 2426, 9, 5, 25095, 6924, 4, 29261, 639, 5, 32394, 2426, 16, 5, 7461, 26187, 6, 10, 19035, 317, 9, 9621, 8, 12456, 4, 85, 16, 10, 24633, 9, 5, 11491, 26187, 23, 226, 2126, 10067, 6, 1470, 147, 5, 9880, 2708, 2851, 13735, 352, 1382, 7, 6130, 6552, 625, 3398, 208, 22895, 853, 1827, 11, 504, 4432, 4, 497, 5, 253, 9, 5, 1049, 1305, 36, 463, 11, 10, 2228, 516, 14, 15230, 149, 155, 19638, 8, 5, 2610, 25336, 238, 16, 10, 2007, 6, 2297, 7326, 9577, 9, 2708, 4, 2]\n",
      "<s>madeupword0000To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?</s></s>Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.</s>\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "class SQuadDataset(Dataset) :\n",
    "    def __init__(self, task, dataroot, split, task_type, task_category, task_choices, max_seq_length, tokenizer) :\n",
    "        super().__init__(task, dataroot, split, task_type, task_category, task_choices, max_seq_length, tokenizer) \n",
    "    \n",
    "    def preprocess_function(self, examples) :\n",
    "        span_token_question = [span_token + examples[\"question\"][i] for i, question in enumerate(examples[\"question\"])]\n",
    "        tokenized_examples = self.tokenizer(\n",
    "            span_token_question,\n",
    "            examples['context'],\n",
    "            truncation=True,\n",
    "            max_length=max_seq_length,\n",
    "            return_overflowing_tokens=True, \n",
    "            return_offsets_mapping=True, \n",
    "        )\n",
    "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\") \n",
    "        offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "        tokenized_examples[\"start_positions\"] = []\n",
    "        tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "        for i, offsets in enumerate(offset_mapping):\n",
    "            input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "            try :\n",
    "                span_index = input_ids.index(50261) ## must be modified\n",
    "            except :\n",
    "                print(input_ids)\n",
    "                print(tokenizer.decode(tokenized_examples[\"input_ids\"][i]))\n",
    "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "            sample_index = sample_mapping[i]\n",
    "            answers = eval(examples[\"answers\"][sample_index])\n",
    "\n",
    "            if len(answers[\"answer_start\"]) == 0:\n",
    "                tokenized_examples[\"start_positions\"].append(span_index)\n",
    "                tokenized_examples[\"end_positions\"].append(span_index)\n",
    "            else:\n",
    "                start_char = answers[\"answer_start\"][0]\n",
    "                end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "                token_start_index = 0\n",
    "                while sequence_ids[token_start_index] != 1 :\n",
    "                    token_start_index += 1\n",
    "\n",
    "                token_end_index = len(input_ids) - 1\n",
    "                while sequence_ids[token_end_index] != 1 :\n",
    "                    token_end_index -= 1\n",
    "\n",
    "                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                    tokenized_examples[\"start_positions\"].append(span_index)\n",
    "                    tokenized_examples[\"end_positions\"].append(span_index)\n",
    "                else:\n",
    "                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                        token_start_index += 1\n",
    "                    tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                    while offsets[token_end_index][1] >= end_char:\n",
    "                        token_end_index -= 1\n",
    "                    tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "        return {k : v for k, v in tokenized_examples.items()}    \n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "DatasetFactory = {\n",
    "    \"CoLA\": CoLADataset,\n",
    "    \"MultiRC\": MultiRCDataset,\n",
    "    \"SocialIQA\": SocialIQADataset,\n",
    "    \"CommonsenseQA\": CommonsenseQADataset,\n",
    "    \"Squad1.1\": SQuadDataset\n",
    "}\n",
    "\n",
    "class RobertaMuppetTokenizer(RobertaTokenizerFast):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_file,\n",
    "        merges_file,\n",
    "        errors=\"replace\",\n",
    "        bos_token=\"<s>\",\n",
    "        eos_token=\"</s>\",\n",
    "        sep_token=\"</s>\",\n",
    "        cls_token=\"<s>\",\n",
    "        unk_token=\"<unk>\",\n",
    "        pad_token=\"<pad>\",\n",
    "        mask_token=\"<mask>\",\n",
    "        span_token=\"madeupword0000\",\n",
    "        mc_token=\"madeupword0001\",\n",
    "        cls_task_token = \"madeupword0002\",\n",
    "        add_prefix_space=False,\n",
    "        **kwargs):\n",
    "        mrc_token = AddedToken(span_token, lstrip=False, rstrip=False) if isinstance(span_token, str) else span_token\n",
    "        com_token = AddedToken(mc_token, lstrip=False, rstrip=False) if isinstance(mc_token, str) else mc_token\n",
    "        cls_task_token = AddedToken(cls_task_token, lstrip=False, rstrip=False) if isinstance(cls_task_token, str) else cls_task_token\n",
    "        special_tokens_dict  = { \"additional_special_tokens\" : [mrc_token, com_token, cls_task_token]}\n",
    "        \n",
    "        super().__init__(\n",
    "            vocab_file=vocab_file,\n",
    "            merges_file=merges_file,\n",
    "            errors=errors,\n",
    "            bos_token=bos_token,\n",
    "            eos_token=eos_token,\n",
    "            unk_token=unk_token,\n",
    "            sep_token=sep_token,\n",
    "            cls_token=cls_token,\n",
    "            pad_token=pad_token,\n",
    "            mask_token=mask_token,\n",
    "            cls_task_token=cls_task_token,\n",
    "            add_prefix_space=add_prefix_space,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.add_special_tokens(special_tokens_dict)\n",
    "        #model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    def build_inputs_with_special_tokens(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        \n",
    "        cls = [self.cls_token_id]\n",
    "        sep = [self.sep_token_id]\n",
    "        if len(token_ids_0) > 0 and token_ids_0[0] in self.additional_special_tokens_ids:\n",
    "            if token_ids_1 is None:\n",
    "                return token_ids_0 + [self.sep_token_id]\n",
    "            return token_ids_0 + sep + sep + token_ids_1 + sep\n",
    "            \n",
    "        if token_ids_1 is None:\n",
    "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
    "        \n",
    "        return cls + token_ids_0 + sep + sep + token_ids_1 + sep\n",
    "\n",
    "\n",
    "\n",
    "task = \"Squad1.1\"\n",
    "#dataroot = \"classification/CoLA\"\n",
    "#dataroot = \"commonsense/socialIQA\"\n",
    "dataroot = \"mrc/squad1.1\"\n",
    "# dataroot = \"mrc/multirc\"\n",
    "task_choices = None\n",
    "max_seq_length = 1024\n",
    "task_type='span'\n",
    "task_category='mrc'\n",
    "tokenizer = RobertaMuppetTokenizer.from_pretrained('roberta-base')\n",
    "split = \"trainval\"\n",
    "squad = DatasetFactory[task](task, dataroot, split, task_type, task_category, task_choices, max_seq_length, tokenizer)\n",
    "processed = squad()\n",
    "print(processed['train']['input_ids'][0])\n",
    "print(tokenizer.decode(processed['train']['input_ids'][0]))\n",
    "print(\"hello\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_2.0",
   "language": "python",
   "name": "tf_2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
